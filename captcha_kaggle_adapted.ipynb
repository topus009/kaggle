{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CAPTCHA OCR Model\\n\n",
        "## Kaggle Adapted - С автоматической проверкой путей\\n\n",
        "\\n\n",
        "Этот ноутбук автоматически найдет пути к данным!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\"\"\"\n",
        "CAPTCHA OCR Model - Adapted from Kaggle notebook\n",
        "Версия для современного TensorFlow и локального запуска\n",
        "\"\"\"\n",
        "\n",
        "# ========================================\n",
        "# КОНСТАНТЫ И НАСТРОЙКИ\n",
        "# ========================================\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Путь к данным - ДЛЯ KAGGLE.COM\n",
        "LABELS_FILE = \"../input/russian-captcha-images-base64/labels.csv\"\n",
        "IMG_FOLDER = \"../input/russian-captcha-images-base64/translit/images\"\n",
        "\n",
        "# Параметры обучения\n",
        "EPOCHS = 15  # Увеличено для лучшей точности\n",
        "BATCH_SIZE = 16\n",
        "TEST_SIZE = 0.1\n",
        "RANDOM_STATE = 42\n",
        "EARLY_STOPPING_PATIENCE = 10\n",
        "\n",
        "# Параметры датасета (для тестирования на малом объеме)\n",
        "USE_FULL_DATASET = True  # True = полный датасет, False = только первые 1000\n",
        "MAX_SAMPLES = 1000  # Если USE_FULL_DATASET=False\n",
        "\n",
        "# Параметры модели\n",
        "MAX_SEQUENCE_LENGTH = 7\n",
        "IMG_WIDTH = 200\n",
        "IMG_HEIGHT = 60\n",
        "IMG_CHANNELS = 3\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"КОНФИГУРАЦИЯ\")\n",
        "print(\"=\"*60)\n",
        "print(f\"EPOCHS: {EPOCHS}\")\n",
        "print(f\"BATCH_SIZE: {BATCH_SIZE}\")\n",
        "print(f\"USE_FULL_DATASET: {USE_FULL_DATASET}\")\n",
        "if not USE_FULL_DATASET:\n",
        "    print(f\"MAX_SAMPLES: {MAX_SAMPLES}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ========================================\n",
        "# БЛОК 1: Загрузка данных\n",
        "# ========================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n[БЛОК 1] Загрузка данных...\")\n",
        "\n",
        "df = pd.read_csv(LABELS_FILE, header=None, encoding='utf-8', delimiter=';', names=['text', 'filename'])\n",
        "data = {row.text: row.filename for row in df.itertuples()}\n",
        "\n",
        "print(f\"Загружено {len(data)} записей\")\n",
        "\n",
        "# Ограничиваем для быстрого теста\n",
        "if not USE_FULL_DATASET:\n",
        "    items = list(data.items())[:MAX_SAMPLES]\n",
        "    data = {k: v for k, v in items}\n",
        "    print(f\"Ограничено до {len(data)} записей для быстрого теста\")\n",
        "\n",
        "# ========================================\n",
        "# БЛОК 2: Создание словаря символов\n",
        "# ========================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n[БЛОК 2] Создание словаря символов...\")\n",
        "\n",
        "characters = sorted(set(''.join(data.keys())))\n",
        "char_to_num = {v: i for i, v in enumerate(characters)}\n",
        "num_to_char = {str(i): v for i, v in enumerate(characters)}\n",
        "num_to_char['-1'] = 'UKN'  # Для CTC декодирования\n",
        "\n",
        "print(f\"Алфавит ({len(characters)} символов): {characters}\")\n",
        "\n",
        "# ========================================\n",
        "# БЛОК 3: Функция предобработки изображения\n",
        "# ========================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n[БЛОК 3] Определение функций предобработки...\")\n",
        "\n",
        "def encode_single_sample(filename):\n",
        "    \"\"\"Загружает и предобрабатывает изображение\"\"\"\n",
        "    img_path = os.path.join(IMG_FOLDER, filename)\n",
        "    img = tf.io.read_file(img_path)\n",
        "    \n",
        "    try:\n",
        "        img = tf.io.decode_png(img, channels=3)\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка с файлом {img_path}: {e}\")\n",
        "        # Пробуем JPEG\n",
        "        img = tf.io.decode_jpeg(img, channels=3)\n",
        "    \n",
        "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "    img = tf.transpose(img, perm=[1, 0, 2])  # (width, height, channels)\n",
        "    \n",
        "    return img.numpy()\n",
        "\n",
        "print(\"Функции предобработки определены\")\n",
        "\n",
        "# ========================================\n",
        "# БЛОК 4: Создание обучающего и валидационного датасетов\n",
        "# ========================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n[БЛОК 4] Создание обучающих данных...\")\n",
        "\n",
        "def create_train_and_validation_datasets():\n",
        "    X, y = [], []\n",
        "    \n",
        "    items = list(data.items())\n",
        "    \n",
        "    if USE_FULL_DATASET:\n",
        "        # Полный датасет: первые 10000 + последние 10000\n",
        "        if len(items) > 20000:\n",
        "            train_dataset = items[:10000] + items[-10000:]\n",
        "            test_dataset = items[10000:-10000]\n",
        "        else:\n",
        "            # Если мало данных, используем все\n",
        "            train_dataset = items\n",
        "            test_dataset = []\n",
        "    else:\n",
        "        # Быстрый тест на малой выборке\n",
        "        train_dataset = items\n",
        "        test_dataset = []\n",
        "    \n",
        "    y_texts, filenames = zip(*train_dataset)\n",
        "    \n",
        "    print(f\"Кодируем {len(filenames)} изображений...\")\n",
        "    \n",
        "    # Сначала создаем y_list, фильтруя невалидные данные\n",
        "    y_list = []\n",
        "    valid_filenames = []\n",
        "    skipped = 0\n",
        "    \n",
        "    for label, filename in zip(y_texts, filenames):\n",
        "        try:\n",
        "            encoded = [char_to_num[c] for c in label]\n",
        "            y_list.append(encoded)\n",
        "            valid_filenames.append(filename)\n",
        "        except KeyError as e:\n",
        "            skipped += 1\n",
        "            if skipped <= 3:  # Показываем первые 3 ошибки\n",
        "                print(f\"Предупреждение: пропускаем '{label}' из-за неизвестного символа\")\n",
        "    \n",
        "    if skipped > 0:\n",
        "        print(f\"Всего пропущено: {skipped} записей с невалидными символами\")\n",
        "    \n",
        "    # Теперь загружаем только валидные изображения\n",
        "    X = np.asarray([encode_single_sample(fn) for fn in valid_filenames])\n",
        "    \n",
        "    # Применяем padding\n",
        "    y = tf.keras.preprocessing.sequence.pad_sequences(y_list, MAX_SEQUENCE_LENGTH, padding='post', value=-1)\n",
        "    \n",
        "    print(f\"Форма X: {X.shape}\")\n",
        "    print(f\"Форма y: {y.shape}\")\n",
        "    \n",
        "    # Разделение на train/val\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X, y, test_size=TEST_SIZE, shuffle=True, random_state=RANDOM_STATE\n",
        "    )\n",
        "    \n",
        "    return X_train, X_val, y_train, y_val, test_dataset\n",
        "\n",
        "X_train, X_val, y_train, y_val, test_dataset = create_train_and_validation_datasets()\n",
        "\n",
        "print(f\"Разделение на train/val: X_train={X_train.shape}, X_val={X_val.shape}\")\n",
        "\n",
        "# ========================================\n",
        "# БЛОК 5: Визуализация данных\n",
        "# ========================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n[БЛОК 5] Визуализация примеров...\")\n",
        "\n",
        "# Проверяем что данные загружены\n",
        "if 'X_train' not in globals() or 'X_val' not in globals():\n",
        "    print(\"❌ ОШИБКА: Данные не загружены!\")\n",
        "    print(\"Вы должны сначала запустить ячейки БЛОК 1-4 (cells 1-5)\")\n",
        "    print(\"Пожалуйста, запустите все предыдущие ячейки по порядку\")\n",
        "    raise NameError(\"X_train не определен. Запустите ячейки 1-5 перед этой ячейкой.\")\n",
        "\n",
        "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
        "axes[0, 0].imshow(X_train[0], cmap='gray')\n",
        "axes[0, 0].set_title(f'X_train[0]: {list(y_train[0])}')\n",
        "axes[0, 0].axis('off')\n",
        "\n",
        "axes[0, 1].imshow(X_train[min(135, len(X_train)-1)], cmap='gray')\n",
        "axes[0, 1].set_title(f'X_train[135]: {list(y_train[min(135, len(y_train)-1)])}')\n",
        "axes[0, 1].axis('off')\n",
        "\n",
        "axes[1, 0].imshow(X_val[0], cmap='gray')\n",
        "axes[1, 0].set_title(f'X_val[0]: {list(y_val[0])}')\n",
        "axes[1, 0].axis('off')\n",
        "\n",
        "axes[1, 1].imshow(X_val[min(23, len(X_val)-1)], cmap='gray')\n",
        "axes[1, 1].set_title(f'X_val[23]: {list(y_val[min(23, len(y_val)-1)])}')\n",
        "axes[1, 1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "os.makedirs(\"output\", exist_ok=True)\n",
        "plt.savefig('output/dataset_samples.png')\n",
        "print(\"Примеры сохранены в output/dataset_samples.png\")\n",
        "\n",
        "# ========================================\n",
        "# БЛОК 6: Определение CTCLayer\n",
        "# ========================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n[БЛОК 6] Определение CTCLayer...\")\n",
        "\n",
        "class CTCLayer(layers.Layer):\n",
        "    \"\"\"CTC Loss Layer для обучения\"\"\"\n",
        "    def __init__(self, name=None):\n",
        "        super().__init__(name=name)\n",
        "        self.loss_fn = keras.backend.ctc_batch_cost\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
        "        input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
        "        \n",
        "        input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
        "        labels_mask = 1 - tf.cast(tf.equal(y_true, -1), dtype=\"int64\")\n",
        "        labels_length = tf.reduce_sum(labels_mask, axis=1)\n",
        "        \n",
        "        loss = self.loss_fn(y_true, y_pred, input_length, tf.expand_dims(labels_length, -1))\n",
        "        self.add_loss(loss)\n",
        "        \n",
        "        return y_pred\n",
        "\n",
        "print(\"CTCLayer определен\")\n",
        "\n",
        "# ========================================\n",
        "# БЛОК 7: Построение модели\n",
        "# ========================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n[БЛОК 7] Построение модели...\")\n",
        "\n",
        "def build_model():\n",
        "    \"\"\"Создает OCR CNN-LSTM модель\"\"\"\n",
        "    input_img = layers.Input(shape=(IMG_WIDTH, IMG_HEIGHT, IMG_CHANNELS), name=\"image\", dtype=\"float32\") \n",
        "    labels = layers.Input(name=\"label\", shape=(MAX_SEQUENCE_LENGTH,), dtype=\"float32\")\n",
        "\n",
        "    # Первый conv блок\n",
        "    x = layers.Conv2D(32, (3, 3), activation=\"relu\", kernel_initializer=\"he_normal\", \n",
        "                      padding=\"same\", name=\"Conv1\")(input_img)\n",
        "    x = layers.MaxPooling2D((2, 2), name=\"pool1\")(x)\n",
        "\n",
        "    # Второй conv блок\n",
        "    x = layers.Conv2D(64, (3, 3), activation=\"relu\", kernel_initializer=\"he_normal\", \n",
        "                      padding=\"same\", name=\"Conv2\")(x)\n",
        "    x = layers.MaxPooling2D((2, 2), name=\"pool2\")(x)\n",
        "\n",
        "    # Reshape для RNN\n",
        "    x = layers.Reshape(target_shape=(50, 960), name=\"reshape\")(x)\n",
        "    x = layers.Dense(64, activation=\"relu\", name=\"dense1\")(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "\n",
        "    # RNN слои\n",
        "    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True, dropout=0.25))(x)\n",
        "    x = layers.Bidirectional(layers.LSTM(64, return_sequences=True, dropout=0.25))(x)\n",
        "\n",
        "    # Output layer\n",
        "    x = layers.Dense(len(characters) + 1, activation=\"softmax\", name=\"dense2\")(x)\n",
        "\n",
        "    # CTC layer\n",
        "    output = CTCLayer(name=\"ctc_loss\")(labels, x)\n",
        "\n",
        "    # Определяем модель\n",
        "    model = keras.models.Model(inputs=[input_img, labels], outputs=output, name=\"ocr_cnn_lstm_model\")\n",
        "    \n",
        "    # Компилируем\n",
        "    model.compile(optimizer=keras.optimizers.Adam())\n",
        "    return model\n",
        "\n",
        "model = build_model()\n",
        "print(\"\\nСводка модели:\")\n",
        "model.summary()\n",
        "\n",
        "# ========================================\n",
        "# БЛОК 8: Создание TensorFlow datasets\n",
        "# ========================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n[БЛОК 8] Создание TensorFlow datasets...\")\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "train_dataset = train_dataset.map(\n",
        "    lambda x, y: {'image': x, 'label': y}\n",
        ").batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "validation_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
        "validation_dataset = validation_dataset.map(\n",
        "    lambda x, y: {'image': x, 'label': y}\n",
        ").batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "print(\"Datasets созданы\")\n",
        "\n",
        "# ========================================\n",
        "# БЛОК 9: Обучение модели\n",
        "# ========================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n[БЛОК 9] Обучение модели...\")\n",
        "\n",
        "early_stopping = keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_loss\", \n",
        "    patience=EARLY_STOPPING_PATIENCE, \n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "print(f\"Начинаем обучение на {EPOCHS} эпохах...\")\n",
        "history = model.fit(\n",
        "    train_dataset, \n",
        "    validation_data=validation_dataset, \n",
        "    epochs=EPOCHS, \n",
        "    callbacks=[early_stopping],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\nОбучение завершено!\")\n",
        "\n",
        "# ========================================\n",
        "# БЛОК 10: Визуализация истории обучения\n",
        "# ========================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n[БЛОК 10] Визуализация истории обучения...\")\n",
        "\n",
        "import os\n",
        "os.makedirs(\"output\", exist_ok=True)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('CTC Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig('output/training_history.png')\n",
        "print(\"График сохранен в output/training_history.png\")\n",
        "\n",
        "# ========================================\n",
        "# БЛОК 11: Создание prediction model\n",
        "# ========================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n[БЛОК 11] Создание prediction model...\")\n",
        "\n",
        "prediction_model = keras.models.Model(\n",
        "    model.inputs[0],  # Используем первый input модели\n",
        "    model.get_layer(name=\"dense2\").output\n",
        ")\n",
        "\n",
        "prediction_model.summary()\n",
        "\n",
        "# ========================================\n",
        "# БЛОК 12: Сохранение модели\n",
        "# ========================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n[БЛОК 12] Сохранение модели...\")\n",
        "\n",
        "# Создаем папку output если её нет\n",
        "import os\n",
        "os.makedirs(\"output\", exist_ok=True)\n",
        "\n",
        "# Сохранение в разных форматах\n",
        "prediction_model.save(\"output/model.h5\")\n",
        "print(\"✓ Модель сохранена в output/model.h5\")\n",
        "\n",
        "prediction_model.save(\"output/model.keras\")\n",
        "print(\"✓ Модель сохранена в output/model.keras\")\n",
        "\n",
        "# Экспорт в SavedModel формат\n",
        "try:\n",
        "    prediction_model.export(\"output/model\")\n",
        "    print(\"✓ Модель экспортирована в output/model (SavedModel format)\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠ Не удалось экспортировать в SavedModel: {e}\")\n",
        "\n",
        "# Экспорт в TFLite (для мобильных устройств)\n",
        "try:\n",
        "    converter = tf.lite.TFLiteConverter.from_keras_model(prediction_model)\n",
        "    tflite_model = converter.convert()\n",
        "    with open(\"output/model.tflite\", \"wb\") as f:\n",
        "        f.write(tflite_model)\n",
        "    print(\"✓ Модель экспортирована в output/model.tflite (TensorFlow Lite)\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠ Не удалось экспортировать в TFLite: {e}\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"Все модели сохранены в папку output/\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "# ========================================\n",
        "# БЛОК 13: Тестирование на валидационной выборке\n",
        "# ========================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n[БЛОК 13] Тестирование на валидационной выборке...\")\n",
        "\n",
        "y_pred = prediction_model.predict(X_val, verbose=1)\n",
        "y_pred_decoded = keras.backend.ctc_decode(\n",
        "    y_pred, \n",
        "    input_length=np.ones(X_val.shape[0]) * 50, \n",
        "    greedy=True\n",
        ")[0][0][:, :MAX_SEQUENCE_LENGTH].numpy()\n",
        "\n",
        "print(\"\\nПримеры предсказаний:\")\n",
        "for i in range(min(10, len(y_val))):\n",
        "    pred_chars = [num_to_char[str(x)] if x > -1 else '' for x in y_pred_decoded[i]]\n",
        "    true_chars = [num_to_char[str(int(x))] if int(x) > -1 else '' for x in y_val[i]]\n",
        "    \n",
        "    pred_text = ''.join([c for c in pred_chars if c != 'UKN'])\n",
        "    true_text = ''.join([c for c in true_chars if c != 'UKN'])\n",
        "    \n",
        "    match = \"✓\" if pred_text == true_text else \"✗\"\n",
        "    print(f\"{match} True: '{true_text}' | Pred: '{pred_text}'\")\n",
        "\n",
        "# ========================================\n",
        "# БЛОК 14: Вычисление точности\n",
        "# ========================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n[БЛОК 14] Вычисление точности...\")\n",
        "\n",
        "correct = 0\n",
        "total = len(y_val)\n",
        "\n",
        "for i in range(len(y_val)):\n",
        "    pred_chars = [num_to_char[str(x)] if x > -1 else '' for x in y_pred_decoded[i]]\n",
        "    true_chars = [num_to_char[str(int(x))] if int(x) > -1 else '' for x in y_val[i]]\n",
        "    \n",
        "    pred_text = ''.join([c for c in pred_chars if c not in ['UKN', '']])\n",
        "    true_text = ''.join([c for c in true_chars if c not in ['UKN', '']])\n",
        "    \n",
        "    if pred_text == true_text:\n",
        "        correct += 1\n",
        "\n",
        "accuracy = correct / total * 100\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"ТОЧНОСТЬ: {accuracy:.2f}% ({correct}/{total})\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "print(\"\\nОбучение завершено успешно!\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
