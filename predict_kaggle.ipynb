{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîÆ CAPTCHA OCR Prediction - Kaggle Version\n",
    "\n",
    "–≠—Ç–æ—Ç notebook –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –Ω–∞ Kaggle —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏.\n",
    "\n",
    "## –ß—Ç–æ –Ω—É–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º:\n",
    "1. –ó–∞–≥—Ä—É–∑–∏—Ç—å –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –≤ `input/` (—Ñ–∞–π–ª `model.keras`)\n",
    "2. –ó–∞–≥—Ä—É–∑–∏—Ç—å —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ `input/` (–ø–∞–ø–∫–∞ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏)\n",
    "3. –ó–∞–ø—É—Å—Ç–∏—Ç—å –≤—Å–µ —è—á–µ–π–∫–∏\n",
    "\n",
    "## –°—Ç—Ä—É–∫—Ç—É—Ä–∞ input:\n",
    "```\n",
    "input/\n",
    "‚îú‚îÄ‚îÄ model.keras          # –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å\n",
    "‚îî‚îÄ‚îÄ test_images/         # –ü–∞–ø–∫–∞ —Å —Ç–µ—Å—Ç–æ–≤—ã–º–∏ CAPTCHA\n",
    "    ‚îú‚îÄ‚îÄ captcha_001.png\n",
    "    ‚îú‚îÄ‚îÄ captcha_002.png\n",
    "    ‚îî‚îÄ‚îÄ ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ò–º–ø–æ—Ä—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import glob\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "print(\"üì¶ –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ\")\n",
    "print(f\"TensorFlow –≤–µ—Ä—Å–∏—è: {tf.__version__}\")\n",
    "print(f\"Keras –≤–µ—Ä—Å–∏—è: {keras.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥–µ–ª–∏\n",
    "IMG_WIDTH = 200\n",
    "IMG_HEIGHT = 60\n",
    "MAX_SEQUENCE_LENGTH = 7\n",
    "\n",
    "# –ü—É—Ç–∏ –∫ –¥–∞–Ω–Ω—ã–º\n",
    "MODEL_PATH = \"/kaggle/input/model.keras\"\n",
    "TEST_IMAGES_PATH = \"/kaggle/input/test_images/\"\n",
    "OUTPUT_PATH = \"/kaggle/working/predictions/\"\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "print(\"‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\")\n",
    "print(f\"–ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏: {MODEL_PATH}\")\n",
    "print(f\"–ü—É—Ç—å –∫ —Ç–µ—Å—Ç–æ–≤—ã–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º: {TEST_IMAGES_PATH}\")\n",
    "print(f\"–ü–∞–ø–∫–∞ –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Ñ–∞–π–ª—ã\n",
    "print(\"üìÅ –°–æ–¥–µ—Ä–∂–∏–º–æ–µ input:\")\n",
    "for root, dirs, files in os.walk(\"/kaggle/input/\"):\n",
    "    level = root.replace(\"/kaggle/input/\", \"\").count(os.sep)\n",
    "    indent = \" \" * 2 * level\n",
    "    print(f\"{indent}{os.path.basename(root)}/\")\n",
    "    subindent = \" \" * 2 * (level + 1)\n",
    "    for file in files[:10]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 10 —Ñ–∞–π–ª–æ–≤\n",
    "        print(f\"{subindent}{file}\")\n",
    "    if len(files) > 10:\n",
    "        print(f\"{subindent}... –∏ –µ—â–µ {len(files) - 10} —Ñ–∞–π–ª–æ–≤\")\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –º–æ–¥–µ–ª–∏\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    print(f\"\\n‚úÖ –ú–æ–¥–µ–ª—å –Ω–∞–π–¥–µ–Ω–∞: {MODEL_PATH}\")\n",
    "    model_size = os.path.getsize(MODEL_PATH) / (1024 * 1024)\n",
    "    print(f\"   –†–∞–∑–º–µ—Ä: {model_size:.2f} MB\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {MODEL_PATH}\")\n",
    "    print(\"   –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –∑–∞–≥—Ä—É–∑–∏–ª–∏ —Ñ–∞–π–ª model.keras –≤ input/\")\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\n",
    "if os.path.exists(TEST_IMAGES_PATH):\n",
    "    image_files = glob.glob(os.path.join(TEST_IMAGES_PATH, \"*.png\")) + \\\n",
    "                  glob.glob(os.path.join(TEST_IMAGES_PATH, \"*.jpg\")) + \\\n",
    "                  glob.glob(os.path.join(TEST_IMAGES_PATH, \"*.jpeg\"))\n",
    "    print(f\"\\n‚úÖ –¢–µ—Å—Ç–æ–≤—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–∞–π–¥–µ–Ω—ã: {len(image_files)} —Ñ–∞–π–ª–æ–≤\")\n",
    "    if image_files:\n",
    "        print(f\"   –ü—Ä–∏–º–µ—Ä—ã: {[os.path.basename(f) for f in image_files[:5]]}\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå –ü–∞–ø–∫–∞ —Å —Ç–µ—Å—Ç–æ–≤—ã–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {TEST_IMAGES_PATH}\")\n",
    "    print(\"   –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –∑–∞–≥—Ä—É–∑–∏–ª–∏ –ø–∞–ø–∫—É —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –≤ input/test_images/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Å–ª–æ–≤–∞—Ä–µ–π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path):\n",
    "    \"\"\"–ó–∞–≥—Ä—É–∂–∞–µ—Ç –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å\"\"\"\n",
    "    try:\n",
    "        print(f\"üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏–∑ {model_path}...\")\n",
    "        model = keras.models.load_model(model_path)\n",
    "        print(\"‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_char_mappings():\n",
    "    \"\"\"–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–ª–æ–≤–∞—Ä–∏ —Å–∏–º–≤–æ–ª–æ–≤\"\"\"\n",
    "    # –ü—ã—Ç–∞–µ–º—Å—è –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∞–ª—Ñ–∞–≤–∏—Ç –∏–∑ –¥–∞–Ω–Ω—ã—Ö –æ–±—É—á–µ–Ω–∏—è\n",
    "    characters = None\n",
    "    \n",
    "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Ñ–∞–π–ª —Å –º–µ—Ç–∫–∞–º–∏ –≤ input\n",
    "    possible_labels = [\n",
    "        \"/kaggle/input/test_images/labels.csv\",\n",
    "        \"/kaggle/input/labels.csv\", \n",
    "        \"data/labels.csv\",\n",
    "        \"labels.csv\"\n",
    "    ]\n",
    "    \n",
    "    for labels_file in possible_labels:\n",
    "        if os.path.exists(labels_file):\n",
    "            try:\n",
    "                print(f\"üîç –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∞–ª—Ñ–∞–≤–∏—Ç –∏–∑ {labels_file}...\")\n",
    "                with open(labels_file, 'r', encoding='utf-8') as f:\n",
    "                    lines = f.readlines()\n",
    "                \n",
    "                # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ —Å–∏–º–≤–æ–ª—ã –∏–∑ –¥–∞–Ω–Ω—ã—Ö\n",
    "                all_chars = set()\n",
    "                for line in lines:\n",
    "                    text = line.split(';')[0].strip()\n",
    "                    all_chars.update(text)\n",
    "                \n",
    "                characters = sorted(all_chars)\n",
    "                print(f\"‚úÖ –ê–ª—Ñ–∞–≤–∏—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω –∏–∑ –¥–∞–Ω–Ω—ã—Ö: {len(characters)} —Å–∏–º–≤–æ–ª–æ–≤\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {labels_file}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∏–∑ –¥–∞–Ω–Ω—ã—Ö, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–ª–Ω—ã–π –∞–ª—Ñ–∞–≤–∏—Ç\n",
    "    if characters is None:\n",
    "        print(\"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∞–ª—Ñ–∞–≤–∏—Ç –∏–∑ –¥–∞–Ω–Ω—ã—Ö, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–ª–Ω—ã–π –Ω–∞–±–æ—Ä\")\n",
    "        characters = sorted(set('–∞–±–≤–≥–¥–µ–∂–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—ä—ã—å—ç—é—è0123456789'))\n",
    "    \n",
    "    char_to_num = {v: i for i, v in enumerate(characters)}\n",
    "    num_to_char = {str(i): v for i, v in enumerate(characters)}\n",
    "    num_to_char['-1'] = 'UKN'  # –î–ª—è CTC –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è\n",
    "    \n",
    "    print(f\"üìù –ê–ª—Ñ–∞–≤–∏—Ç ({len(characters)} —Å–∏–º–≤–æ–ª–æ–≤): {characters}\")\n",
    "    return char_to_num, num_to_char\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    model = load_model(MODEL_PATH)\n",
    "    char_to_num, num_to_char = load_char_mappings()\n",
    "    \n",
    "    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –º–æ–¥–µ–ª–∏\n",
    "    if model:\n",
    "        print(\"\\nüèóÔ∏è –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏:\")\n",
    "        model.summary()\n",
    "else:\n",
    "    print(\"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å - —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω\")\n",
    "    model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üñºÔ∏è –§—É–Ω–∫—Ü–∏–∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path, img_width=IMG_WIDTH, img_height=IMG_HEIGHT):\n",
    "    \"\"\"–ü—Ä–µ–¥–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –º–æ–¥–µ–ª–∏\"\"\"\n",
    "    try:\n",
    "        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\n",
    "        img = tf.io.read_file(image_path)\n",
    "        \n",
    "        # –î–µ–∫–æ–¥–∏—Ä—É–µ–º (–ø—Ä–æ–±—É–µ–º PNG, –∑–∞—Ç–µ–º JPEG)\n",
    "        try:\n",
    "            img = tf.io.decode_png(img, channels=3)\n",
    "        except:\n",
    "            img = tf.io.decode_jpeg(img, channels=3)\n",
    "        \n",
    "        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ float32 –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º\n",
    "        img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "        \n",
    "        # –¢—Ä–∞–Ω—Å–ø–æ–Ω–∏—Ä—É–µ–º (width, height, channels)\n",
    "        img = tf.transpose(img, perm=[1, 0, 2])\n",
    "        \n",
    "        # –ò–∑–º–µ–Ω—è–µ–º —Ä–∞–∑–º–µ—Ä –µ—Å–ª–∏ –Ω—É–∂–Ω–æ\n",
    "        img = tf.image.resize(img, [img_width, img_height])\n",
    "        \n",
    "        # –î–æ–±–∞–≤–ª—è–µ–º batch dimension\n",
    "        img = tf.expand_dims(img, 0)\n",
    "        \n",
    "        return img.numpy()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def decode_predictions(predictions, num_to_char):\n",
    "    \"\"\"–î–µ–∫–æ–¥–∏—Ä—É–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ –≤ —Ç–µ–∫—Å—Ç\"\"\"\n",
    "    # –ü–æ–ª—É—á–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é\n",
    "    input_len = np.ones(predictions.shape[0]) * predictions.shape[1]\n",
    "    results = tf.keras.backend.ctc_decode(predictions, input_length=input_len, greedy=True)[0][0]\n",
    "    \n",
    "    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Ç–µ–∫—Å—Ç\n",
    "    texts = []\n",
    "    for result in results:\n",
    "        text = \"\"\n",
    "        for idx in result:\n",
    "            if idx != -1:  # -1 –æ–∑–Ω–∞—á–∞–µ—Ç padding\n",
    "                char = num_to_char.get(str(idx.numpy()), '')\n",
    "                if char != 'UKN':\n",
    "                    text += char\n",
    "        texts.append(text)\n",
    "    \n",
    "    return texts\n",
    "\n",
    "print(\"üîß –§—É–Ω–∫—Ü–∏–∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÆ –§—É–Ω–∫—Ü–∏–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_image(model, image_path, char_to_num, num_to_char):\n",
    "    \"\"\"–ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –¥–ª—è –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\"\"\"\n",
    "    filename = os.path.basename(image_path)\n",
    "    print(f\"üîç –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º: {filename}\")\n",
    "    \n",
    "    # –ü—Ä–µ–¥–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\n",
    "    processed_img = preprocess_image(image_path)\n",
    "    if processed_img is None:\n",
    "        return None\n",
    "    \n",
    "    # –î–µ–ª–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ\n",
    "    try:\n",
    "        predictions = model.predict(processed_img, verbose=0)\n",
    "        predicted_text = decode_predictions(predictions, num_to_char)[0]\n",
    "        \n",
    "        print(f\"üìù –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: '{predicted_text}'\")\n",
    "        return predicted_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {e}\")\n",
    "        return None\n",
    "\n",
    "def visualize_prediction(image_path, predicted_text, save_path=None):\n",
    "    \"\"\"–í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ–º\"\"\"\n",
    "    try:\n",
    "        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
    "        img = Image.open(image_path)\n",
    "        \n",
    "        # –°–æ–∑–¥–∞–µ–º —Ñ–∏–≥—É—Ä—É\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.title(f\"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: '{predicted_text}'\", fontsize=16, fontweight='bold')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, bbox_inches='tight', dpi=150)\n",
    "            print(f\"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {save_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå –û—à–∏–±–∫–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏: {e}\")\n",
    "\n",
    "print(\"üîÆ –§—É–Ω–∫—Ü–∏–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω—ã\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –æ–¥–Ω–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ù–∞—Ö–æ–¥–∏–º –ø–µ—Ä–≤–æ–µ –¥–æ—Å—Ç—É–ø–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è\n",
    "if os.path.exists(TEST_IMAGES_PATH) and model:\n",
    "    image_files = glob.glob(os.path.join(TEST_IMAGES_PATH, \"*.png\")) + \\\n",
    "                  glob.glob(os.path.join(TEST_IMAGES_PATH, \"*.jpg\")) + \\\n",
    "                  glob.glob(os.path.join(TEST_IMAGES_PATH, \"*.jpeg\"))\n",
    "    \n",
    "    if image_files:\n",
    "        test_image = image_files[0]\n",
    "        print(f\"üß™ –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏: {os.path.basename(test_image)}\")\n",
    "        \n",
    "        # –î–µ–ª–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ\n",
    "        predicted_text = predict_single_image(model, test_image, char_to_num, num_to_char)\n",
    "        \n",
    "        if predicted_text is not None:\n",
    "            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
    "            visualize_prediction(test_image, predicted_text)\n",
    "        else:\n",
    "            print(\"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–¥–µ–ª–∞—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ\")\n",
    "    else:\n",
    "        print(\"‚ùå –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –ø–∞–ø–∫–µ test_images\")\n",
    "else:\n",
    "    print(\"‚ùå –ú–æ–¥–µ–ª—å –∏–ª–∏ –ø–∞–ø–∫–∞ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä –ú–∞—Å—Å–æ–≤–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(TEST_IMAGES_PATH) and model:\n",
    "    # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
    "    image_files = glob.glob(os.path.join(TEST_IMAGES_PATH, \"*.png\")) + \\\n",
    "                  glob.glob(os.path.join(TEST_IMAGES_PATH, \"*.jpg\")) + \\\n",
    "                  glob.glob(os.path.join(TEST_IMAGES_PATH, \"*.jpeg\"))\n",
    "    \n",
    "    if image_files:\n",
    "        print(f\"üìä –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º {len(image_files)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π...\")\n",
    "        \n",
    "        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\n",
    "        results = []\n",
    "        successful_predictions = 0\n",
    "        \n",
    "        for i, image_path in enumerate(image_files):\n",
    "            predicted_text = predict_single_image(model, image_path, char_to_num, num_to_char)\n",
    "            \n",
    "            if predicted_text is not None:\n",
    "                results.append({\n",
    "                    'filename': os.path.basename(image_path),\n",
    "                    'prediction': predicted_text,\n",
    "                    'length': len(predicted_text)\n",
    "                })\n",
    "                successful_predictions += 1\n",
    "            else:\n",
    "                results.append({\n",
    "                    'filename': os.path.basename(image_path),\n",
    "                    'prediction': '',\n",
    "                    'length': 0\n",
    "                })\n",
    "            \n",
    "            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {i + 1}/{len(image_files)}\")\n",
    "        \n",
    "        print(f\"\\n‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!\")\n",
    "        print(f\"   –£—Å–ø–µ—à–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {successful_predictions}/{len(image_files)}\")\n",
    "        print(f\"   –ü—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—Ö–∞: {successful_predictions/len(image_files)*100:.1f}%\")\n",
    "        \n",
    "        # –°–æ–∑–¥–∞–µ–º DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏\n",
    "        df_results = pd.DataFrame(results)\n",
    "        \n",
    "        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É\n",
    "        print(\"\\nüìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π:\")\n",
    "        print(f\"   –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {df_results['length'].mean():.1f} —Å–∏–º–≤–æ–ª–æ–≤\")\n",
    "        print(f\"   –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞: {df_results['length'].min()} —Å–∏–º–≤–æ–ª–æ–≤\")\n",
    "        print(f\"   –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞: {df_results['length'].max()} —Å–∏–º–≤–æ–ª–æ–≤\")\n",
    "        \n",
    "        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 10 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "        print(\"\\nüîç –ü–µ—Ä–≤—ã–µ 10 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:\")\n",
    "        display(df_results.head(10))\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –ø–∞–ø–∫–µ test_images\")\n",
    "else:\n",
    "    print(\"‚ùå –ú–æ–¥–µ–ª—å –∏–ª–∏ –ø–∞–ø–∫–∞ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df_results' in locals() and not df_results.empty:\n",
    "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ CSV\n",
    "    csv_path = os.path.join(OUTPUT_PATH, \"predictions.csv\")\n",
    "    df_results.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "    print(f\"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {csv_path}\")\n",
    "    \n",
    "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏\n",
    "    txt_path = os.path.join(OUTPUT_PATH, \"predictions.txt\")\n",
    "    with open(txt_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π CAPTCHA OCR\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "        for _, row in df_results.iterrows():\n",
    "            f.write(f\"{row['filename']}: {row['prediction']}\\n\")\n",
    "    print(f\"üíæ –¢–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {txt_path}\")\n",
    "    \n",
    "    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏\n",
    "    print(\"\\nüñºÔ∏è –ü—Ä–∏–º–µ—Ä—ã –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏:\")\n",
    "    sample_results = df_results.head(5)\n",
    "    \n",
    "    for _, row in sample_results.iterrows():\n",
    "        if row['prediction']:  # –¢–æ–ª—å–∫–æ –¥–ª—è —É—Å–ø–µ—à–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\n",
    "            image_path = os.path.join(TEST_IMAGES_PATH, row['filename'])\n",
    "            if os.path.exists(image_path):\n",
    "                save_path = os.path.join(OUTPUT_PATH, f\"result_{row['filename']}\")\n",
    "                visualize_prediction(image_path, row['prediction'], save_path)\n",
    "    \n",
    "    print(f\"\\n‚úÖ –í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–∞–ø–∫—É: {OUTPUT_PATH}\")\n",
    "    print(\"   - predictions.csv - —Ç–∞–±–ª–∏—Ü–∞ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏\")\n",
    "    print(\"   - predictions.txt - —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏\")\n",
    "    print(\"   - result_*.png - –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå –ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã –ò—Ç–æ–≥–æ–≤–∞—è —Å–≤–æ–¥–∫–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéâ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω—ã!\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if 'df_results' in locals() and not df_results.empty:\n",
    "    total_images = len(df_results)\n",
    "    successful = len(df_results[df_results['prediction'] != ''])\n",
    "    success_rate = successful / total_images * 100\n",
    "    \n",
    "    print(f\"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {total_images}\")\n",
    "    print(f\"‚úÖ –£—Å–ø–µ—à–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {successful}\")\n",
    "    print(f\"üìà –ü—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—Ö–∞: {success_rate:.1f}%\")\n",
    "    print(f\"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {OUTPUT_PATH}\")\n",
    "    \n",
    "    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\n",
    "    print(\"\\nüîç –ü—Ä–∏–º–µ—Ä—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π:\")\n",
    "    sample_predictions = df_results[df_results['prediction'] != ''].head(10)\n",
    "    for _, row in sample_predictions.iterrows():\n",
    "        print(f\"   {row['filename']}: '{row['prediction']}'\")\n",
    "    \n",
    "    if len(sample_predictions) < len(df_results[df_results['prediction'] != '']):\n",
    "        print(f\"   ... –∏ –µ—â–µ {len(df_results[df_results['prediction'] != '']) - len(sample_predictions)} –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–µ –±—ã–ª–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω—ã\")\n",
    "    print(\"   –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞–ª–∏—á–∏–µ –º–æ–¥–µ–ª–∏ –∏ —Ç–µ—Å—Ç–æ–≤—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\")\n",
    "\n",
    "print(\"\\nüöÄ –ì–æ—Ç–æ–≤–æ! –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∞ –Ω–∞ Kaggle.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
  },
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "nbconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}